- Class: meta
  Course: Bootstrap
  Lesson: Bootstrap Lesson
  Author: Gina Norato
  Type: Standard
  Organization: Johns Hopkins Biostatistics
  Version: 2.2.21


- Class: text
  Output: Welcome to the Bootstrap course! Here we will learn about what boostrapping is and how it can be useful in data analysis.

- Class: video
  Output: Bootstrapping is a technique first described by Bradley Efron in a 1979 paper. Would you like to go to the article now? (page will open in your default web browser)
  VideoLink: https://projecteuclid.org/download/pdf_1/euclid.aos/1176344552
    
- Class: text
  Output: In a nutshell, bootstrapping helps us understand the sampling distribution of a particular statistic (sample mean, median, etc.), by drawing samples with replacement from the observed sample. 
  
- Class: text
  Output: When we collect data from a population to obtain a sample, we want to be able to have a measure of variability of this statistic in order to perform proper inference. Often, however, we only have the current, single sample, and getting additional unique samples may be difficult or unreasonable. We can instead take repeated samples from our obtained sample. This mimics the acquisition of multiple samples from the population, and can help us describe the error of our statistic.
  
- Class: text
  Output: Let's look at a simple example to get some intuition.
  
- Class: cmd_question
  Output: Say we have a true population distribution called 'pop' which is normal with mean 0 and standard deviation 2 (n=10000). Generate this sample.
  CorrectAnswer: pop <- rnorm(10000, mean = 0, sd = 2)
  AnswerTests: any_of_exprs('pop <- rnorm(10000, 0, 2)',
                              'pop = rnorm(10000, 0, 2)',
                              'pop <- rnorm(10000, mean=0, sd=2)',
                              'pop = rnorm(10000, mean=0, sd=2)',
                              'pop = rnorm(n=10000, mean=0, sd=2)',
                              'pop <- rnorm(n=10000, mean=0, sd=2)')
  Hint: Type 'pop <- rnorm(10000, 0, 2)'
  
- Class: text
  Output: Let's see what that looks like. We can plot this easily with 'hist'
  
- Class: figure
  Output: Now we have our original population. Now imagine we run a study and end up with a sample of this underlying population that we can investigate. Here we will use the 'sample' function to do this.
  Figure: figure1.R
  FigureType: new
  
- Class: text
  Output: The 'sample' function allows us to sample from a vector, to create a new vector of a certain size, with or without replacement. We will use this function again later, but here we will just use it to generate our original sample. Don't forget you can access more information about this function using ?sample.
  
- Class: cmd_question
  Output: Let's try it out. Generate our 200-observation sample with the command 'samp <- sample(pop, size=200, replace=T)'
  CorrectAnswer: samp <- sample(pop, size=200, replace=T)
  AnswerTests: omnitest('samp <- sample(pop, size=200, replace=T)')
  Hint: Type 'samp <- sample(pop, size=200, replace=T)'
  
- Class: text
  Output: Now we have a sample. Let's plot that too (using 'hist')
  
- Class: figure
  Output: Let's see what the mean and sd are...
  Figure: figure2.R
  FigureType: new
  
- Class: cmd_question
  Output: Calculate the mean and save it as samp.mean
  CorrectAnswer: samp.mean <- mean(samp)
  AnswerTests: any_of_exprs('samp.mean <- mean(samp)', 'samp.mean = mean(samp)')
  Hint: Type 'samp.mean <- mean(samp)'
  
- Class: cmd_question
  Output: Now calculate the standard deviation and save it as samp.sd
  CorrectAnswer: samp.sd <- sd(samp)
  AnswerTests: any_of_exprs('samp.sd <- sd(samp)','samp.sd = sd(samp)')
  Hint: Type 'samp.sd <- sd(samp)'
  
- Class: cmd_question
  Output: Let's print samp.mean to check the value
  CorrectAnswer: samp.mean
  AnswerTests: omnitest('samp.mean')
  Hint: Type 'samp.mean'
  
- Class: cmd_question
  Output: And let's also print samp.sd
  CorrectAnswer: samp.sd
  AnswerTests: omnitest('samp.sd')
  Hint: Type 'samp.sd'

- Class: text
  Output: We see that our mean is pretty close to the population mean, but maybe we want to know how variable our estimate of the mean is, without making any assumptions about distributional form. This is where bootstrapping comes in.

- Class: text
  Output: What we want to do is simulate taking repeated samples from the population by taking repeated samples from the obtained sample. This seems confusing, but let's try it out and get some intuition as we go.
  
- Class: cmd_question
  Output: You should remember the 'sample' function. Let's use that now to get a "bootstrapped" sample from our original sample. We will sample this at the same size of our existing sample (200), and do it with replacement. Call it samp.b1
  CorrectAnswer: samp.b1 <- sample(samp, size=200, replace=T)
  AnswerTests: any_of_exprs('samp.b1 <- sample(samp, size=200, replace=T)',
            'samp.b1 = sample(samp, size=200, replace=T)',
            'samp.b1 = sample(samp,200,replace=T)',
            'samp.b1 <- sample(samp,200,replace=T);,
            'samp.b1 = sample(samp,200,T)',
            'samp.b1 <- sample(samp,200,T)',
            'samp.b1 <- sample(samp,200,replace=TRUE)',
            'samp.b1 = sample(samp,200,replace=TRUE)',
            'samp.b1 <- sample(samp,size=200,replace=TRUE)',
            'samp.b1 = sample(samp,size=200,replace=TRUE)')
  Hint: Type 'samp.b1 <- sample(samp, size=200, replace=T)'
  
- Class: cmd_question
  Output: Now we have a bootstrapped sample, which simulates what would happen if we could take another sample from the population directly. Here we assume that our original sample is representative of the population. Let's calculate the mean and save it as samp.b1mean and see how we did.
  CorrectAnswer: samp.b1mean <- mean(samp.b1)
  AnswerTests: any_of_exprs('samp.b1mean <- mean(samp.b1)', 'samp.mean = mean(samp.b1)')
  Hint: Type 'samp.b1mean <- mean(samp.b1)'
  
- Class: text
  Output: This time, our sample mean is slightly different, which we can also visualize by plotting. 
  
- Class: figure
  Output: Hopefully you are beginning to see the methodology here. With repeated boostrapped samples and repeated calculations of the mean, we can get a distribution of means, which can help give us a better picture of the statistic itself, in relation to the population.
  Figure: figure3.R
  FigureType: new 
  
- Class: cmd_question
  Output: You might be wondering if we have to generate each of these samples individually by hand... Well luckily, we don't have to! There is an R function that can help us, called 'boot'! At the start of the lesson, we installed (if necessary) and loaded this library. Let's look at the help file with '?boot'
  CorrectAnswer: ?boot
  AnswerTests: omnitest('?boot')
  Hint: Type '?boot'
  
- Class: text
  Output: The 'boot' function takes the data, the statistic to be calculated, and the number of samples to be generated as arguments. Typically, we generate as many samples as computationally feasible. Here we will just calculate 1000. I have already created a function called meanFunc to calculate the mean for each sample- 'meanFunc <- function(x,i){mean(x[i])}'
  
- Class: text
  Output: We must do this because the 'boot' function will not just take the usual 'mean' function as an argument. You could create a similar function to calculate other statistics (ie median).
  
- Class: cmd_question
  Output: Generate the samples from 'samp' and assign it to 'boot'- 'boot <- boot(samp,meanFunc,1000)'
  CorrectAnswer: boot <- boot(samp,meanFunc,1000)
  AnswerTests: omnitest('boot <- boot(samp,meanFunc,1000)')
  Hint: Type 'boot <- boot(samp,meanFunc,1000)'
  
- Class: cmd_question
  Output: Now we have a 'boot' object that we can work with. Let's plot it to see what we have
  CorrectAnswer: plot(boot)
  AnswerTests: omnitest('plot(boot)')
  Hint: Type 'plot(boot)'
  
- Class: text
  Output: On the left we see a histogram of the statistic (means) of our bootstrapped samples. We can see that these values all fall very closely around 0, which we know is the true underlying mean of the population. On the right is a normal probability plot or QQ-plot. If the distribution appears to be normal, then using assumptions of normality when creating confidence intervals with the bootstrap standard error would be appropriate.
  
- Class: text
  Output: There are many different methods of calculation of these confidence intervals, all of which are available through the 'boot' library, (and which can be read about further here <http://www.tau.ac.il/~saharon/Boot/10.1.1.133.8405.pdf>). For this example, we will use the percentile method. In the percentile method, the 2.5% and 97.5% quantiles are calculated from the bootstrapped distribution, and form the lower and upper bounds of the confidence interval. This method is robust to the shape of the bootstrapped distribution of the statistic.
  
- Class: cmd_question
  Output: Let's get the confidence interval now- boot.ci(boot,type="perc")
  CorrectAnswer: boot.ci(boot,type="perc")
  AnswerTests: omnitest('boot.ci(boot,type="perc")')
  Hint: Type 'boot.ci(boot,type="perc")'
  
- Class: text
  Output: So now we have a 95% confidence interval for how precise our estimate of the mean is for this population! We can see that this interval does in fact cover the true mean, which is what we would expect.
  
- Class: text
  Output: Even though there is the 'boot' function to help us, it is straightforward enough to also create the bootstrapped samples and confidence intervals with a loop. Instead of walking you through it, I have included the code and an example for you to browse, in the next file.
  
- Class: figure
  Output: Opening html...
  Figure: html viewer.R
  FigureType: new
  
- Class: text
  Output: This example shows us a few things. One is that bootstrapping is incredibly useful for investigating skewed data and skewed or non-normal sampling distributions. It is more robust to deviations from normality since it is only resampling from the obtained data and does not require such strict assumptions to be made. In a similar way, we can also do bootstrapping on distributions that are unknown.

- Class: text
  Output: Bootstrapping also naturally extends to describing statistics such as medians, where we do not have a formula for finding the standard error or confidence intervals. Bootstrapping can also be used on regression coefficients.
  
- Class: text
  Output: One disadvantage of using bootstrapping is that the assumption is made that the sample that is being bootstrapped from is indeed a good representation of the population. If it is too small or is a poor representation, then the bootstrapped samples will not mimic the behavior in the population. It is also not useful for statistics such as minima or maxima.
  
- Class: text
  Output: There is certainly more information that you can learn about the bootstrap through the resources I've provided, but hopefully this lesson gives you the tools you need to get started! Happy bootstrapping!
  



  
  
  
  
  
  
  
  

